{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d1eb8b-83c5-423f-baef-10972992d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e262c33-35e9-4323-b7f9-675132819a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/All-time_Olympic_Games_medal_table\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7aeaf5e-5c09-4e8b-8ecb-f04791475af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_article_title(soup):\n",
    "    \"\"\"Extracts the title of the Wikipedia article from the BeautifulSoup object.\"\"\"\n",
    "    title = soup.find('h1', {'id': 'firstHeading'})\n",
    "    return title.text if title else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d685c23-0688-4a15-9c1a-166a29189cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(soup):\n",
    "    \"\"\"Extracts paragraphs and their headings from the Wikipedia article and maps them to a dictionary.\"\"\"\n",
    "    article_content = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6ad22ba-6004-4b3e-bd1b-da7f5f387228",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:9\u001b[1;36m\u001b[0m\n\u001b[1;33m    if tag.name.startswith('h'):\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "      # Find all headings and paragraphs\n",
    "    headings = soup.find_all(['h2', 'h3', 'h4'])\n",
    "    paragraphs = soup.find_all('p')\n",
    "    \n",
    "    current_heading = 'Introduction'\n",
    "    article_content[current_heading] = []\n",
    "    \n",
    "    for tag in soup.find_all(['h2', 'h3', 'h4', 'p']):\n",
    "        if tag.name.startswith('h'):\n",
    "            # Handle heading\n",
    "            current_heading = tag.text.strip()\n",
    "            article_content[current_heading] = []\n",
    "        elif tag.name == 'p':\n",
    "            # Handle paragraph\n",
    "            if current_heading not in article_content:\n",
    "            article_content[current_heading] = []\n",
    "            article_content[current_heading].append(tag.text.strip())\n",
    "    \n",
    "    return article_content\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "971b2cc3-851b-45fe-b815-a460717e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_wikipedia_links(soup):\n",
    "    \"\"\"Collects all Wikipedia internal links from the BeautifulSoup object.\"\"\"\n",
    "    links = set()\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if href.startswith('/wiki/') and ':' not in href:  # Filters out non-Wikipedia links\n",
    "            links.add(f\"https://en.wikipedia.org{href}\")\n",
    "    return links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89ead176-bd29-494f-9cb3-251ee5fb1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_page(url):\n",
    "    \"\"\"Main function to scrape the Wikipedia page.\"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title = extract_article_title(soup)\n",
    "    \n",
    "    # Extract article text\n",
    "    article_text = extract_article_text(soup)\n",
    "    \n",
    "    # Collect links\n",
    "    links = collect_wikipedia_links(soup)\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'article_text': article_text,\n",
    "        'links': links\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdeda9c-321d-4063-80f2-4b023066ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
